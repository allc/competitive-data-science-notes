{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"Here is no method which outperforms all others for all tasks.\" (No Free Lunch Theorem)\n",
    "\n",
    "# Introduction\n",
    "\n",
    "> Guided by Coursera course \"[How to Win a Data Science Competition: Learn from Top Kagglers](https://www.coursera.org/learn/competitive-data-science)\".\n",
    "\n",
    "## Machine Learning Algorithms\n",
    "\n",
    "- Linear\n",
    "  - *Logistic Regression*\n",
    "  - *Support Vector Machines*\n",
    "  - Good for sparse and high dimension\n",
    "  - Implemented in scikit-learn and Vowpal Wabbit (handle large data)\n",
    "  \n",
    "- Tree-based\n",
    "  - *Decision tree*\n",
    "  - *Random forest*\n",
    "    - Disadvantages\n",
    "      - Does not train well on small dataset\n",
    "      - Problem of interpretability: cannot see or understand the relationship between the response and the independent variables\n",
    "      - Time taken to train random forest may sometimes be huge\n",
    "      - For regression, the range of values response variable can take is determined by the values already available in the training dataset\n",
    "    - Advantages\n",
    "      - Compred to single decision tree, bias remains the same. However, variances decreas, thus decrease the chances of overfitting\n",
    "      - Quick and dirty. Do not have to worry about assumptions of the model or linearity in the dataset\n",
    "  - *GBDT*\n",
    "    - In practice GBDT is used with small learning rate (0.01 < $\\mu$ < 0.1) and large number of trees to get the best results\n",
    "  - Implemented in scikit-learn, XGBoost and Microsoft/LightGBM (later two for higher speed and accuracy)\n",
    "  \n",
    "- *kNN*\n",
    "  - Implemented in scikit-learn\n",
    "  - Based on assumption of closest points have same labels\n",
    "  - Rely on measure of \"closeness\"\n",
    "\n",
    "- *Neural networks (NN)*\n",
    "\n",
    "## References\n",
    "\n",
    "- Barnwal, M. (2017). *Random Forests explained intuitively*. [online] Data Science Central. Available at: https://www.datasciencecentral.com/profiles/blogs/random-forests-explained-intuitively [Accessed 7 Jul. 2019].\n",
    "- Rogozhnikov A. (2016) *Gradient Boosting explained [demonstration]*. [blog] Brilliantly wrong. Available at: http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html [Accessed 8 Jul. 2019]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
